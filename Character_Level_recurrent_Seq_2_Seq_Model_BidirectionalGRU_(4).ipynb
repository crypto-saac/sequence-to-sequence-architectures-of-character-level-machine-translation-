{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crypto-saac/sequence-to-sequence-architectures-of-character-level-machine-translation-/blob/main/Character_Level_recurrent_Seq_2_Seq_Model_BidirectionalGRU_(4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glGgYZSu-XA2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!!unzip fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgLhI19Z-q_l",
        "outputId": "b7e8ed00-58f1-485f-a43e-7ed041edf7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Archive:  fra-eng.zip',\n",
              " '  inflating: _about.txt              ',\n",
              " '  inflating: fra.txt                 ']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = \"fra.txt\""
      ],
      "metadata": {
        "id": "Nv2J1njI-sFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens (num_encoder_tokenes):\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens (num_decoder_tokens):\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs (max_encoder_seq_length):\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs (max_decoder_seq_length):\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfTTQBFL-tT2",
        "outputId": "0368a105-5ee2-46f7-9717-00b13f0d0c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens (num_encoder_tokenes): 70\n",
            "Number of unique output tokens (num_decoder_tokens): 93\n",
            "Max sequence length for inputs (max_encoder_seq_length): 14\n",
            "Max sequence length for outputs (max_decoder_seq_length): 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "encoder = keras.layers.Bidirectional(keras.layers.GRU(latent_dim, return_state=True, return_sequences=True))  # Update return_sequences=True\n",
        "\n",
        "# Process the input sequence\n",
        "#encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
        "encoder_outputs, forward_h, backward_h= encoder(encoder_inputs)\n",
        "\n",
        "# Concatenate the hidden states\n",
        "state_h = keras.layers.Concatenate()([forward_h, backward_h]) \n",
        "#state_c = keras.layers.Concatenate()([forward_c, backward_c])\n",
        "\n",
        "# Define the encoder states\n",
        "encoder_states = [state_h]\n",
        "\n",
        "# Define the decoder inputs\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# Define the decoder GRU\n",
        "decoder_gru = keras.layers.GRU(latent_dim*2, return_sequences=True, return_state=True)\n",
        "\n",
        "# Process the decoder inputs with the initial states from the encoder\n",
        "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "# Define the decoder dense layer\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "XKE9vWz5-vxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ye7QD1w1eC5",
        "outputId": "4213d599-9786-46d4-b8ba-4426694edac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, None, 93) dtype=float32 (created by layer 'dense')>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdwXYzR3-yp2",
        "outputId": "95a73691-64f2-4559-8867-843756975e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "125/125 [==============================] - 13s 27ms/step - loss: 1.1581 - accuracy: 0.7443 - val_loss: 0.8548 - val_accuracy: 0.7537\n",
            "Epoch 2/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.6624 - accuracy: 0.8124 - val_loss: 0.6590 - val_accuracy: 0.8102\n",
            "Epoch 3/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.5527 - accuracy: 0.8402 - val_loss: 0.5845 - val_accuracy: 0.8292\n",
            "Epoch 4/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.5037 - accuracy: 0.8531 - val_loss: 0.5500 - val_accuracy: 0.8403\n",
            "Epoch 5/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.4714 - accuracy: 0.8617 - val_loss: 0.5252 - val_accuracy: 0.8453\n",
            "Epoch 6/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.4455 - accuracy: 0.8679 - val_loss: 0.5029 - val_accuracy: 0.8513\n",
            "Epoch 7/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.4217 - accuracy: 0.8747 - val_loss: 0.4850 - val_accuracy: 0.8567\n",
            "Epoch 8/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.3995 - accuracy: 0.8805 - val_loss: 0.4656 - val_accuracy: 0.8628\n",
            "Epoch 9/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.3781 - accuracy: 0.8868 - val_loss: 0.4535 - val_accuracy: 0.8661\n",
            "Epoch 10/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.3570 - accuracy: 0.8931 - val_loss: 0.4387 - val_accuracy: 0.8711\n",
            "Epoch 11/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.3358 - accuracy: 0.8990 - val_loss: 0.4316 - val_accuracy: 0.8742\n",
            "Epoch 12/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.3160 - accuracy: 0.9049 - val_loss: 0.4207 - val_accuracy: 0.8775\n",
            "Epoch 13/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.2951 - accuracy: 0.9115 - val_loss: 0.4134 - val_accuracy: 0.8798\n",
            "Epoch 14/100\n",
            "125/125 [==============================] - 3s 21ms/step - loss: 0.2753 - accuracy: 0.9174 - val_loss: 0.4057 - val_accuracy: 0.8829\n",
            "Epoch 15/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.2564 - accuracy: 0.9228 - val_loss: 0.4042 - val_accuracy: 0.8838\n",
            "Epoch 16/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.2370 - accuracy: 0.9283 - val_loss: 0.4049 - val_accuracy: 0.8849\n",
            "Epoch 17/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.2200 - accuracy: 0.9332 - val_loss: 0.4044 - val_accuracy: 0.8863\n",
            "Epoch 18/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.2019 - accuracy: 0.9384 - val_loss: 0.4086 - val_accuracy: 0.8872\n",
            "Epoch 19/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.1860 - accuracy: 0.9432 - val_loss: 0.4141 - val_accuracy: 0.8865\n",
            "Epoch 20/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.1989 - accuracy: 0.9390 - val_loss: 0.4098 - val_accuracy: 0.8882\n",
            "Epoch 21/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.1635 - accuracy: 0.9499 - val_loss: 0.4172 - val_accuracy: 0.8885\n",
            "Epoch 22/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.1487 - accuracy: 0.9541 - val_loss: 0.4265 - val_accuracy: 0.8886\n",
            "Epoch 23/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.1351 - accuracy: 0.9584 - val_loss: 0.4401 - val_accuracy: 0.8875\n",
            "Epoch 24/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.1242 - accuracy: 0.9616 - val_loss: 0.4516 - val_accuracy: 0.8870\n",
            "Epoch 25/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.1133 - accuracy: 0.9648 - val_loss: 0.4592 - val_accuracy: 0.8864\n",
            "Epoch 26/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.1036 - accuracy: 0.9678 - val_loss: 0.4674 - val_accuracy: 0.8870\n",
            "Epoch 27/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.0952 - accuracy: 0.9701 - val_loss: 0.4789 - val_accuracy: 0.8863\n",
            "Epoch 28/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.0878 - accuracy: 0.9723 - val_loss: 0.4895 - val_accuracy: 0.8865\n",
            "Epoch 29/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.0805 - accuracy: 0.9746 - val_loss: 0.5019 - val_accuracy: 0.8844\n",
            "Epoch 30/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0743 - accuracy: 0.9765 - val_loss: 0.5128 - val_accuracy: 0.8854\n",
            "Epoch 31/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0693 - accuracy: 0.9779 - val_loss: 0.5204 - val_accuracy: 0.8857\n",
            "Epoch 32/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0640 - accuracy: 0.9796 - val_loss: 0.5282 - val_accuracy: 0.8857\n",
            "Epoch 33/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0598 - accuracy: 0.9808 - val_loss: 0.5412 - val_accuracy: 0.8839\n",
            "Epoch 34/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.0570 - accuracy: 0.9815 - val_loss: 0.5544 - val_accuracy: 0.8849\n",
            "Epoch 35/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0535 - accuracy: 0.9826 - val_loss: 0.5603 - val_accuracy: 0.8853\n",
            "Epoch 36/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0505 - accuracy: 0.9834 - val_loss: 0.5680 - val_accuracy: 0.8840\n",
            "Epoch 37/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0489 - accuracy: 0.9837 - val_loss: 0.5695 - val_accuracy: 0.8848\n",
            "Epoch 38/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.0461 - accuracy: 0.9846 - val_loss: 0.5824 - val_accuracy: 0.8853\n",
            "Epoch 39/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.0449 - accuracy: 0.9848 - val_loss: 0.5872 - val_accuracy: 0.8841\n",
            "Epoch 40/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0431 - accuracy: 0.9853 - val_loss: 0.5959 - val_accuracy: 0.8844\n",
            "Epoch 41/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0422 - accuracy: 0.9855 - val_loss: 0.5975 - val_accuracy: 0.8837\n",
            "Epoch 42/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0405 - accuracy: 0.9860 - val_loss: 0.6019 - val_accuracy: 0.8842\n",
            "Epoch 43/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0399 - accuracy: 0.9861 - val_loss: 0.6081 - val_accuracy: 0.8846\n",
            "Epoch 44/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.0396 - accuracy: 0.9861 - val_loss: 0.6128 - val_accuracy: 0.8846\n",
            "Epoch 45/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0385 - accuracy: 0.9864 - val_loss: 0.6204 - val_accuracy: 0.8838\n",
            "Epoch 46/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.0381 - accuracy: 0.9863 - val_loss: 0.6214 - val_accuracy: 0.8844\n",
            "Epoch 47/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0369 - accuracy: 0.9868 - val_loss: 0.6257 - val_accuracy: 0.8839\n",
            "Epoch 48/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0367 - accuracy: 0.9867 - val_loss: 0.6303 - val_accuracy: 0.8841\n",
            "Epoch 49/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0364 - accuracy: 0.9869 - val_loss: 0.6307 - val_accuracy: 0.8848\n",
            "Epoch 50/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0358 - accuracy: 0.9869 - val_loss: 0.6333 - val_accuracy: 0.8849\n",
            "Epoch 51/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0352 - accuracy: 0.9869 - val_loss: 0.6340 - val_accuracy: 0.8849\n",
            "Epoch 52/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0348 - accuracy: 0.9871 - val_loss: 0.6411 - val_accuracy: 0.8843\n",
            "Epoch 53/100\n",
            "125/125 [==============================] - 3s 21ms/step - loss: 0.0346 - accuracy: 0.9871 - val_loss: 0.6377 - val_accuracy: 0.8845\n",
            "Epoch 54/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0342 - accuracy: 0.9871 - val_loss: 0.6405 - val_accuracy: 0.8850\n",
            "Epoch 55/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.0340 - accuracy: 0.9872 - val_loss: 0.6453 - val_accuracy: 0.8848\n",
            "Epoch 56/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0335 - accuracy: 0.9874 - val_loss: 0.6432 - val_accuracy: 0.8856\n",
            "Epoch 57/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0333 - accuracy: 0.9872 - val_loss: 0.6439 - val_accuracy: 0.8856\n",
            "Epoch 58/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0332 - accuracy: 0.9872 - val_loss: 0.6484 - val_accuracy: 0.8839\n",
            "Epoch 59/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0332 - accuracy: 0.9872 - val_loss: 0.6533 - val_accuracy: 0.8843\n",
            "Epoch 60/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0330 - accuracy: 0.9873 - val_loss: 0.6509 - val_accuracy: 0.8850\n",
            "Epoch 61/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0328 - accuracy: 0.9874 - val_loss: 0.6562 - val_accuracy: 0.8848\n",
            "Epoch 62/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0326 - accuracy: 0.9874 - val_loss: 0.6521 - val_accuracy: 0.8855\n",
            "Epoch 63/100\n",
            "125/125 [==============================] - 3s 21ms/step - loss: 0.0323 - accuracy: 0.9874 - val_loss: 0.6543 - val_accuracy: 0.8852\n",
            "Epoch 64/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0320 - accuracy: 0.9874 - val_loss: 0.6586 - val_accuracy: 0.8847\n",
            "Epoch 65/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0318 - accuracy: 0.9875 - val_loss: 0.6592 - val_accuracy: 0.8852\n",
            "Epoch 66/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0336 - accuracy: 0.9872 - val_loss: 0.6630 - val_accuracy: 0.8839\n",
            "Epoch 67/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0329 - accuracy: 0.9871 - val_loss: 0.6616 - val_accuracy: 0.8852\n",
            "Epoch 68/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0321 - accuracy: 0.9873 - val_loss: 0.6594 - val_accuracy: 0.8853\n",
            "Epoch 69/100\n",
            "125/125 [==============================] - 2s 20ms/step - loss: 0.0320 - accuracy: 0.9874 - val_loss: 0.6648 - val_accuracy: 0.8855\n",
            "Epoch 70/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0313 - accuracy: 0.9874 - val_loss: 0.6671 - val_accuracy: 0.8850\n",
            "Epoch 71/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0309 - accuracy: 0.9875 - val_loss: 0.6654 - val_accuracy: 0.8852\n",
            "Epoch 72/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.0303 - accuracy: 0.9876 - val_loss: 0.6662 - val_accuracy: 0.8851\n",
            "Epoch 73/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0303 - accuracy: 0.9876 - val_loss: 0.6605 - val_accuracy: 0.8860\n",
            "Epoch 74/100\n",
            "125/125 [==============================] - 3s 21ms/step - loss: 0.0303 - accuracy: 0.9876 - val_loss: 0.6638 - val_accuracy: 0.8858\n",
            "Epoch 75/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0301 - accuracy: 0.9874 - val_loss: 0.6612 - val_accuracy: 0.8860\n",
            "Epoch 76/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0296 - accuracy: 0.9876 - val_loss: 0.6663 - val_accuracy: 0.8850\n",
            "Epoch 77/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0299 - accuracy: 0.9875 - val_loss: 0.6693 - val_accuracy: 0.8854\n",
            "Epoch 78/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0299 - accuracy: 0.9877 - val_loss: 0.6727 - val_accuracy: 0.8858\n",
            "Epoch 79/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0297 - accuracy: 0.9877 - val_loss: 0.6665 - val_accuracy: 0.8860\n",
            "Epoch 80/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0299 - accuracy: 0.9875 - val_loss: 0.6778 - val_accuracy: 0.8845\n",
            "Epoch 81/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0304 - accuracy: 0.9875 - val_loss: 0.6752 - val_accuracy: 0.8846\n",
            "Epoch 82/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0299 - accuracy: 0.9875 - val_loss: 0.6725 - val_accuracy: 0.8848\n",
            "Epoch 83/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0297 - accuracy: 0.9875 - val_loss: 0.6746 - val_accuracy: 0.8850\n",
            "Epoch 84/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0296 - accuracy: 0.9876 - val_loss: 0.6743 - val_accuracy: 0.8858\n",
            "Epoch 85/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0294 - accuracy: 0.9875 - val_loss: 0.6766 - val_accuracy: 0.8848\n",
            "Epoch 86/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0293 - accuracy: 0.9877 - val_loss: 0.6735 - val_accuracy: 0.8853\n",
            "Epoch 87/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0290 - accuracy: 0.9876 - val_loss: 0.6787 - val_accuracy: 0.8852\n",
            "Epoch 88/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0286 - accuracy: 0.9877 - val_loss: 0.6767 - val_accuracy: 0.8853\n",
            "Epoch 89/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0286 - accuracy: 0.9877 - val_loss: 0.6772 - val_accuracy: 0.8863\n",
            "Epoch 90/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0286 - accuracy: 0.9877 - val_loss: 0.6841 - val_accuracy: 0.8850\n",
            "Epoch 91/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0286 - accuracy: 0.9875 - val_loss: 0.6771 - val_accuracy: 0.8858\n",
            "Epoch 92/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0279 - accuracy: 0.9879 - val_loss: 0.6928 - val_accuracy: 0.8849\n",
            "Epoch 93/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0280 - accuracy: 0.9878 - val_loss: 0.6848 - val_accuracy: 0.8846\n",
            "Epoch 94/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0285 - accuracy: 0.9876 - val_loss: 0.6771 - val_accuracy: 0.8852\n",
            "Epoch 95/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0283 - accuracy: 0.9876 - val_loss: 0.6862 - val_accuracy: 0.8846\n",
            "Epoch 96/100\n",
            "125/125 [==============================] - 2s 19ms/step - loss: 0.0284 - accuracy: 0.9876 - val_loss: 0.6861 - val_accuracy: 0.8846\n",
            "Epoch 97/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0285 - accuracy: 0.9875 - val_loss: 0.6870 - val_accuracy: 0.8851\n",
            "Epoch 98/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0284 - accuracy: 0.9875 - val_loss: 0.6763 - val_accuracy: 0.8857\n",
            "Epoch 99/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0284 - accuracy: 0.9876 - val_loss: 0.6909 - val_accuracy: 0.8849\n",
            "Epoch 100/100\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0283 - accuracy: 0.9875 - val_loss: 0.6868 - val_accuracy: 0.8852\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f440062b490>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "predictions = model.predict([encoder_input_data, decoder_input_data])\n",
        "\n",
        "# Convert predictions to text\n",
        "predicted_texts = []\n",
        "for prediction in predictions:\n",
        "    predicted_text = \"\"\n",
        "    for timestep in prediction:\n",
        "        # Get index of highest probability\n",
        "        char_index = np.argmax(timestep)\n",
        "        # Convert index to character\n",
        "        char = target_characters[char_index]\n",
        "        # Append character to predicted text\n",
        "        predicted_text += char\n",
        "        # Stop if end sequence character is generated\n",
        "        if char == \"\\n\":\n",
        "            break\n",
        "    predicted_texts.append(predicted_text)\n",
        "\n",
        "# Convert target texts to list of lists\n",
        "target_texts_list = [list(text) for text in target_texts]\n",
        "\n",
        "# Convert predicted texts to list of lists\n",
        "predicted_texts_list = [list(text) for text in predicted_texts]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = corpus_bleu([[text] for text in target_texts_list], predicted_texts_list)\n",
        "print(\"GRU:\")\n",
        "print(\"BLEU score:\", bleu_score)"
      ],
      "metadata": {
        "id": "_eSFBPYpQOml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e46d626-b92f-4047-857a-cd619e4b6692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 6ms/step\n",
            "GRU:\n",
            "BLEU score: 0.7802928075728157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CR1_l60BQ9t-"
      }
    }
  ]
}